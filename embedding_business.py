"""
file/json ì•ˆì— ìˆëŠ” *_business.json íŒŒì¼ì„ ì½ì–´ì„œ
1) business_and_revenue í…ìŠ¤íŠ¸ ìš”ì•½
2) ìš”ì•½ë¬¸ì„ SBERT, FinBERTë¡œ ì„ë² ë”©
3) ChromaDB(chromaDB/chroma_business)ì— ì €ì¥

ìƒì„±ë˜ëŠ” Chroma ì»¬ë ‰ì…˜:
- business_sbert   : SBERT ì„ë² ë”©
- business_finbert : FinBERT ì„ë² ë”©
"""

import json
from pathlib import Path
from typing import List, Dict, Optional

import torch
from transformers import pipeline, AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer

import chromadb
from chromadb.config import Settings


# -----------------------------
# ê²½ë¡œ ì„¤ì •
# -----------------------------

BASE_DIR = Path(__file__).resolve().parent
JSON_DIR = BASE_DIR / "file" / "json"
CHROMA_DIR = BASE_DIR / "chromaDB" / "chroma_business"
CHROMA_DIR.mkdir(parents=True, exist_ok=True)


# -----------------------------
# 0. business.jsonì—ì„œ í…ìŠ¤íŠ¸ êº¼ë‚´ê¸° í—¬í¼
# -----------------------------

def get_business_text_from_data(data: dict) -> str:
    """
    business.json êµ¬ì¡°ê°€ ì—¬ëŸ¬ ê°€ì§€ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
    - ë£¨íŠ¸ì— ë¬¸ìì—´ business_and_revenueê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - ë£¨íŠ¸ì— dictí˜• business_and_revenueê°€ ìˆìœ¼ë©´ ì—°ë„ë³„ë¡œ í•©ì¹¨
    - business_by_yearê°€ ìˆìœ¼ë©´ ê·¸ ì•ˆì˜ business_and_revenueë“¤ì„ í•©ì¹¨
    ìµœì¢…ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ì„ ë°˜í™˜.
    """
    # 1) ë£¨íŠ¸ì˜ business_and_revenueê°€ ë¬¸ìì—´ì¸ ê²½ìš°
    root_text = data.get("business_and_revenue")
    if isinstance(root_text, str) and root_text.strip():
        return root_text.strip()

    # 2) ë£¨íŠ¸ì˜ business_and_revenueê°€ dictì¸ ê²½ìš° (ì˜ˆ: {"2019": "...", "2020": "..."} í˜•íƒœ)
    if isinstance(root_text, dict):
        pieces = []
        for year, txt in sorted(root_text.items(), key=lambda x: x[0]):
            if isinstance(txt, str) and txt.strip():
                pieces.append(f"[{year}]\n{txt.strip()}")
        if pieces:
            return "\n\n".join(pieces)

    # 3) business_by_year êµ¬ì¡°ê°€ ìˆëŠ” ê²½ìš°
    by_year = data.get("business_by_year")
    if isinstance(by_year, dict):
        pieces = []
        for year, info in sorted(by_year.items(), key=lambda x: x[0]):
            if not isinstance(info, dict):
                continue
            txt = info.get("business_and_revenue", "")
            if isinstance(txt, str) and txt.strip():
                pieces.append(f"[{year}]\n{txt.strip()}")
        if pieces:
            return "\n\n".join(pieces)

    # 4) ìœ„ ì¼€ì´ìŠ¤ ë‹¤ ì•ˆ ë§ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
    return ""


# -----------------------------
# 1. í…ìŠ¤íŠ¸ chunking ìœ í‹¸
# -----------------------------

def chunk_text(text: str, max_chars: int = 3000, overlap: int = 200) -> List[str]:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ summarization ëª¨ë¸ìš©ìœ¼ë¡œ ì˜ë¼ì£¼ëŠ” ë‹¨ìˆœ í•¨ìˆ˜.
    - max_chars: í•œ chunk ìµœëŒ€ ë¬¸ì ìˆ˜
    - overlap : ì• chunkì™€ ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜ (ë¬¸ë§¥ ëŠê¹€ ë°©ì§€ìš©)
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text]

    chunks = []
    start = 0
    n = len(text)

    while start < n:
        end = start + max_chars
        if end >= n:
            chunks.append(text[start:])
            break

        # ë„ˆë¬´ ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ëŠì§€ ì•Šë„ë¡, ë’¤ìª½ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ë§ˆì¹¨í‘œ(.) ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ë³´ê¸°
        cut = text.rfind(".", start, end)
        if cut == -1 or cut < start + max_chars * 0.5:
            cut = end

        chunks.append(text[start:cut].strip())
        start = max(cut - overlap, 0)

    return chunks


# -----------------------------
# 2. Summarization íŒŒì´í”„ë¼ì¸
# -----------------------------

def get_summarizer(model_name: str = "facebook/bart-large-cnn"):
    """
    Hugging Face summarization íŒŒì´í”„ë¼ì¸ ì¤€ë¹„.
    í•„ìš”í•˜ë©´ model_nameì„ ë°”ê¿”ì„œ ë‹¤ë¥¸ ìš”ì•½ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥.
    """
    summarizer = pipeline(
        "summarization",
        model=model_name,
        tokenizer=model_name,
    )
    return summarizer


def summarize_long_text(
    summarizer,
    text: str,
    chunk_max_chars: int = 3000,
    overlap: int = 200,
    max_length: int = 200,
    min_length: int = 60,
) -> str:
    """
    1) ê¸´ í…ìŠ¤íŠ¸ë¥¼ chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ 
    2) chunkë§ˆë‹¤ 1ì°¨ ìš”ì•½
    3) (ë©”ëª¨ë¦¬ ì´ìŠˆ ë°©ì§€ë¥¼ ìœ„í•´) 1ì°¨ ìš”ì•½ë“¤ì„ ê·¸ëƒ¥ í•©ì³ì„œ ìµœì¢… ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
    """
    if not text or not text.strip():
        return ""

    chunks = chunk_text(text, max_chars=chunk_max_chars, overlap=overlap)

    partial_summaries: List[str] = []
    for i, ch in enumerate(chunks, start=1):
        print(f"    [SUM] chunk {i}/{len(chunks)} (len={len(ch)})")
        result = summarizer(
            ch,
            max_length=max_length,
            min_length=min_length,
            do_sample=False,
        )[0]["summary_text"]
        partial_summaries.append(result.strip())

    if len(partial_summaries) == 1:
        return partial_summaries[0]

    # ğŸ”´ ë‘ ë²ˆì§¸ ìš”ì•½ì€ MPS OOM ìœ„í—˜ì´ í¬ë¯€ë¡œ ìƒëµ
    combined = "\n".join(partial_summaries)
    print(f"    [SUM] combined summary length (no second pass): {len(combined)}")
    return combined.strip()



# -----------------------------
# 3. SBERT / FinBERT ì„ë² ë”© ì¤€ë¹„
# -----------------------------

def get_sbert_model(model_name: str = "sentence-transformers/all-mpnet-base-v2") -> SentenceTransformer:
    """
    SBERT ê³„ì—´ SentenceTransformer ëª¨ë¸ ë¡œë“œ.
    """
    return SentenceTransformer(model_name)


def get_finbert_model(model_name: str = "ProsusAI/finbert"):
    """
    FinBERT(ì¼ë°˜ BERT ëª¨ë¸)ë¥¼ ë¡œë“œí•˜ê³ ,
    mean pooling ë°©ì‹ìœ¼ë¡œ ë¬¸ì¥ ì„ë² ë”©ì„ ìƒì„±í•  ì˜ˆì •.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    model.eval()
    return tokenizer, model, device


def embed_with_sbert(sbert_model: SentenceTransformer, text: str) -> List[float]:
    """
    SBERTë¡œ í…ìŠ¤íŠ¸ ì„ë² ë”©.
    SentenceTransformerëŠ” ë‚´ë¶€ì ìœ¼ë¡œ mean poolingê¹Œì§€ í•´ ì¤Œ.
    """
    emb = sbert_model.encode(text, convert_to_numpy=True)
    return emb.tolist()


def embed_with_finbert(tokenizer, model, device: str, text: str) -> List[float]:
    """
    FinBERT(BERT)ë¡œ í…ìŠ¤íŠ¸ ì„ë² ë”©.
    - last_hidden_stateì˜ mean pooling ì‚¬ìš© (í† í° í‰ê· )
    """
    if not text or not text.strip():
        return []

    encoded = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,  # í•„ìš”ì‹œ ì¡°ì •
        padding="max_length",
    ).to(device)

    with torch.no_grad():
        outputs = model(**encoded)
        # [batch, seq_len, hidden_size] â†’ [batch, hidden_size]
        token_embeddings = outputs.last_hidden_state  # (1, L, H)
        sentence_embedding = token_embeddings.mean(dim=1)[0]  # (H,)

    return sentence_embedding.cpu().tolist()


# -----------------------------
# 4. ChromaDB ì´ˆê¸°í™”
# -----------------------------

def get_chroma_collections():
    """
    Chroma persistent client ìƒì„± í›„,
    SBERT / FinBERTìš© ì»¬ë ‰ì…˜ ë‘ ê°œë¥¼ ì¤€ë¹„.
    """
    client = chromadb.PersistentClient(path=str(CHROMA_DIR))

    coll_sbert = client.get_or_create_collection(
        name="business_sbert",
        metadata={"description": "Summarized business section embeddings (SBERT)"}
    )

    coll_finbert = client.get_or_create_collection(
        name="business_finbert",
        metadata={"description": "Summarized business section embeddings (FinBERT)"}
    )

    return coll_sbert, coll_finbert


# -----------------------------
# 5. main: JSON â†’ ìš”ì•½ â†’ ì„ë² ë”© â†’ Chroma
# -----------------------------

def process_business_files(
    summarizer_model_name: str = "facebook/bart-large-cnn",
    sbert_model_name: str = "sentence-transformers/all-mpnet-base-v2",
    finbert_model_name: str = "ProsusAI/finbert",
):
    # ëª¨ë¸ ë¡œë“œ
    print("[INIT] Loading summarization model...")
    summarizer = get_summarizer(summarizer_model_name)

    print("[INIT] Loading SBERT model...")
    sbert_model = get_sbert_model(sbert_model_name)

    print("[INIT] Loading FinBERT model...")
    finbert_tokenizer, finbert_model, finbert_device = get_finbert_model(finbert_model_name)

    print("[INIT] Connecting to ChromaDB...")
    coll_sbert, coll_finbert = get_chroma_collections()

    business_files = sorted(JSON_DIR.glob("*_business.json"))
    print(f"[INFO] Found {len(business_files)} business.json files in {JSON_DIR}")

    for idx, path in enumerate(business_files, start=1):
        print(f"\n[{idx}/{len(business_files)}] Processing {path.name}")

        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        symbol = data.get("symbol") or path.stem.replace("_business", "")
        cik = data.get("cik")
        company_name = data.get("company_name")

        # ë£¨íŠ¸ì— filing_* ì •ë³´ê°€ ì—†ë‹¤ë©´, business_by_yearì—ì„œ ê°€ì¥ ìµœê·¼ ì—°ë„ ë©”íƒ€ë°ì´í„° í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
        business_by_year = data.get("business_by_year", {})
        filing_type = data.get("filing_type")
        filing_date = data.get("filing_date")
        filing_url = data.get("filing_url")

        if (filing_type is None or filing_date is None or filing_url is None) and isinstance(business_by_year, dict) and business_by_year:
            latest_year = sorted(business_by_year.keys())[-1]
            latest_meta = business_by_year.get(latest_year, {})
            filing_type = filing_type or latest_meta.get("filing_type")
            filing_date = filing_date or latest_meta.get("filing_date")
            filing_url = filing_url or latest_meta.get("filing_url")

        # ---- ì—¬ê¸°ì„œë¶€í„° ìˆ˜ì •ëœ ë¶€ë¶„: í…ìŠ¤íŠ¸ ì¶”ì¶œ ----
        text = get_business_text_from_data(data)

        if not text or not text.strip():
            print("  [WARN] business text is empty. Skip.")
            continue
        # ---- ìˆ˜ì • ë ----

        # 1) ìš”ì•½
        summary = summarize_long_text(summarizer, text)
        print(f"  [OK] Summary length: {len(summary)} chars")

        # 2) SBERT ì„ë² ë”©
        sbert_vec = embed_with_sbert(sbert_model, summary)
        # 3) FinBERT ì„ë² ë”©
        finbert_vec = embed_with_finbert(finbert_tokenizer, finbert_model, finbert_device, summary)

        # 4) ê³µí†µ ë©”íƒ€ë°ì´í„°
        metadata: Dict[str, Optional[str]] = {
            "symbol": symbol,
            "cik": cik,
            "company_name": company_name,
            "filing_type": filing_type,
            "filing_date": filing_date,
            "filing_url": filing_url,
            "source": "10-K Item 1 Business (summarized)",
        }

        # idëŠ” symbol+filing_date ì¡°í•©ìœ¼ë¡œ ìœ ë‹ˆí¬í•˜ê²Œ
        doc_id = f"{symbol}_{filing_date}" if filing_date else symbol

        # SBERT ì»¬ë ‰ì…˜ì— ì¶”ê°€
        coll_sbert.add(
            ids=[doc_id],
            documents=[summary],
            embeddings=[sbert_vec],
            metadatas=[metadata],
        )
        print("  [CHROMA] Upserted into business_sbert")

        # FinBERT ì»¬ë ‰ì…˜ì— ì¶”ê°€
        if finbert_vec:
            coll_finbert.add(
                ids=[doc_id],
                documents=[summary],
                embeddings=[finbert_vec],
                metadatas=[metadata],
            )
            print("  [CHROMA] Upserted into business_finbert")
        else:
            print("  [WARN] FinBERT embedding is empty. Skipped.")

    # ---------- JSON ëˆ„ë½ ê¸°ì—… ì²´í¬ ----------
    numerical_files = sorted(JSON_DIR.glob("*_numerical.json"))

    business_symbols = {p.stem.replace("_business", "") for p in business_files}
    numerical_symbols = {p.stem.replace("_numerical", "") for p in numerical_files}

    missing_business = sorted(numerical_symbols - business_symbols)

    print("\n[CHECK] JSON consistency check")
    print(f"  numerical.json count : {len(numerical_symbols)}")
    print(f"  business.json count  : {len(business_symbols)}")

    if missing_business:
        print(f"  [WARN] {len(missing_business)} companies have numerical JSON but NO business JSON:")
        for sym in missing_business:
            print(f"    - {sym}")
    else:
        print("  [INFO] All companies with numerical JSON also have business JSON.")

    # ---------- ChromaDB ì €ì¥ ì™„ë£Œ ----------
    print("\n[INFO] Done. All embeddings stored in", CHROMA_DIR)


if __name__ == "__main__":
    # í•„ìš”í•˜ë©´ ì•„ë˜ ëª¨ë¸ ì´ë¦„ì„ í”„ë¡œì íŠ¸ì— ë§ê²Œ ë°”ê¿”ë„ ë¨
    SUMMARIZER_MODEL = "facebook/bart-large-cnn"
    SBERT_MODEL = "sentence-transformers/all-mpnet-base-v2"
    FINBERT_MODEL = "ProsusAI/finbert"

    process_business_files(
        summarizer_model_name=SUMMARIZER_MODEL,
        sbert_model_name=SBERT_MODEL,
        finbert_model_name=FINBERT_MODEL,
    )
